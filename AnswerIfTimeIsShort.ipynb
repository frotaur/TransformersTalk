{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Transformers from scratch (almost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch,torch.nn as nn,torch.nn.functional as F, torch.optim as optim, math\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import TokenTexth5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, attn_length, n_heads, dropout=0.1,device='cpu'): # Define the sub-layers here\n",
    "        super().__init__()\n",
    "        self.attn_length = attn_length\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.q_maker = nn.Linear(embed_dim, n_heads*embed_dim, device=device)\n",
    "        self.k_maker = nn.Linear(embed_dim, n_heads*embed_dim, device=device)\n",
    "        self.v_maker = nn.Linear(embed_dim, n_heads*embed_dim, device=device)\n",
    "\n",
    "        # mask\n",
    "        self.register_buffer('mask',torch.tril(torch.ones((attn_length,attn_length),device=device))) # (attn_length,attn_length) upper triangular mask\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.project_out = nn.Linear(n_heads*embed_dim,embed_dim,device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x): # Apply the different layers\n",
    "        \"\"\"\n",
    "            Args : \n",
    "            x : (B,T,D)\n",
    "\n",
    "            Return : Tensor (B,T,D)\n",
    "        \"\"\"\n",
    "        B,T,D = x.shape\n",
    "\n",
    "        assert T<=self.attn_length, \"Input is too long, limit size to attention lenght.\"\n",
    "        # Generate Q,K,V :\n",
    "\n",
    "        Q = self.q_maker(x).reshape(B,T,self.n_heads,D) # (B,T,n_heads,D)\n",
    "        K = self.k_maker(x).reshape(B,T,self.n_heads,D)  # (B,T,n_heads,D)\n",
    "        V = self.v_maker(x).reshape(B,T,self.n_heads,D)  # (B,T,n_heads,D)\n",
    "\n",
    "        Q = Q.permute(0,2,1,3) # (B,n_heads,T,D)\n",
    "        K = K.permute(0,2,1,3) # (B,n_heads,T,D)\n",
    "        V = V.permute(0,2,1,3) # (B,n_heads,T,D)\n",
    "        \n",
    "        \n",
    "        # Compatibility matrix :\n",
    "        C = Q @ K.transpose(-1,-2) * 1./(math.sqrt(D)) # (B,T,T)\n",
    "        C = torch.masked_fill(C,self.mask[:T,:T]==0, float('-inf')) # C : (B,T,T), mask : (T, T)\n",
    "\n",
    "        # Apply softmax\n",
    "        C = F.softmax(C,dim=-1)\n",
    "\n",
    "        C=self.dropout(C)\n",
    "\n",
    "        attention = C @ V # (B,n_heads,T,D)\n",
    "\n",
    "        attention = attention.permute(0,2,1,3).reshape(B,T,self.n_heads*D)\n",
    "\n",
    "        return self.project_out(attention) # (B,T,D)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_test =  MaskedSelfAttention(embed_dim=6,attn_length=10, n_heads = 2)\n",
    "\n",
    "in_tens = torch.randn((1,8,6))\n",
    "\n",
    "out_tens = attn_test(in_tens)# (B,T,D)\n",
    "\n",
    "assert out_tens.shape==in_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP layer, easy part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple feedforward with two layers. Blows up by a factor of 4 the embed_dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,embed_dim, mlp_ratio=4., device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.Linear(embed_dim, int(mlp_ratio*embed_dim),device=device)\n",
    "        self.ln2 = nn.Linear(int(mlp_ratio*embed_dim),embed_dim,device=device)\n",
    "\n",
    "        self.nonlin = nn.GELU()\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Args :\n",
    "            x : (*,embed_dim)\n",
    "\n",
    "            Returns : (*,embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.ln2(self.nonlin(self.ln1(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 10, 5])\n",
      "outshape :  torch.Size([2, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "testmlp = MLP(embed_dim=5)\n",
    "\n",
    "test_input = torch.randn((2,10,5))\n",
    "\n",
    "print('input shape: ', test_input.shape)\n",
    "print('outshape : ', testmlp(test_input).shape)\n",
    "# print('output : ', testmlp(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Decoder' Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim,attn_length, n_heads, dropout=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, device=device)\n",
    "        self.attn = MaskedSelfAttention(embed_dim,attn_length, n_heads, dropout, device=device)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, device=device)\n",
    "        self.mlp= MLP(embed_dim, device=device)\n",
    "        self.mlp_dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args :\n",
    "        x : (B,T,D) tensor\n",
    "\n",
    "        Returns : (B,T,D) tensor\n",
    "        \"\"\"\n",
    "        x = x+self.attn_dropout(self.attn(self.norm1(x)))\n",
    "        x = x+self.mlp_dropout(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_test =  TransformerBlock(embed_dim=6,attn_length=10, n_heads = 2)\n",
    "\n",
    "in_tens = torch.randn((2,8,6))\n",
    "\n",
    "out_tens = block_test(in_tens)# (B,T,D)\n",
    "\n",
    "assert out_tens.shape==in_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the text transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_layers, vocab_size,embed_dim,attn_length,n_heads, dropout=0.1,device='cpu'):\n",
    "        super().__init__()\n",
    "        self.attn_length = attn_length\n",
    "        self.tok_embedder = nn.Embedding(vocab_size,embed_dim, device=device)\n",
    "        self.pos_embedder = nn.Embedding(attn_length,embed_dim, device=device)\n",
    "\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim,attn_length,n_heads,dropout,device=device) for _ in range(n_layers)])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(embed_dim,device=device)\n",
    "\n",
    "        self.project_final = nn.Linear(embed_dim,vocab_size,bias=False,device=device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Args : \n",
    "            x : (B,T) of longs (tokens)\n",
    "\n",
    "            Returns : (B,T,vocab_size) of logits\n",
    "        \"\"\"\n",
    "        B,T = x.shape\n",
    "\n",
    "        positions = torch.arange(0,T,1, device=x.device) # positions\n",
    "        pos_embedded = self.pos_embedder(positions)\n",
    "        tok_embedded = self.tok_embedder(x) # (B,T,D)\n",
    "\n",
    "        x = pos_embedded+tok_embedded\n",
    "        \n",
    "        for block in self.blocks :\n",
    "            x = block(x) # (B,T,D)\n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        x = self.project_final(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (B,T)) and complete\n",
    "            the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "            Use with model in inference mode (apply model.eval() first)\n",
    "\n",
    "            Args :\n",
    "            idx : (B,T) tensor of context tokens. Mostly, it will be B=1 but can do in parallel also\n",
    "            max_new_tokens : number of tokens to generate on top of the conditioning sequence\n",
    "            temperature : softmax temperature (lower -> more conservative sampling)\n",
    "            do_sample : if True, use multinomial sampling. Otherwise use greedy decoding\n",
    "            top_k : if set to int > 0, only sample from the top k most probable logits\n",
    "\n",
    "            Returns :\n",
    "            (B,T) LongTensor of generated token indices. Must still be decoded by tokenizer.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_next = self.generate_next_token(idx,temperature=temperature,do_sample=do_sample,top_k=top_k)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_next_token(self,idx,temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (B,T)) and return\n",
    "            the next predicted token.\n",
    "            Use with model in inference mode (apply model.eval() first)\n",
    "\n",
    "            Args :\n",
    "            idx : (B,T) tensor of context tokens. Mostly, it will be B=1 but can do in parallel also\n",
    "            temperature : softmax temperature (lower -> more conservative sampling)\n",
    "            do_sample : if True, use multinomial sampling. Otherwise use greedy decoding\n",
    "            top_k : if set to int > 0, only sample from the top k most probable logits\n",
    "\n",
    "            Returns :\n",
    "            next predicted token, Long\n",
    "        \"\"\"\n",
    "        idx_cond = idx if idx.shape[1] <= self.attn_length else idx[:, -self.attn_length:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits = self.forward(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            \n",
    "        # Return sampled index\n",
    "        return idx_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 0.00M tokens, resulting in 44 examples.\n",
      "Example raw : \n",
      " tensor([6279,   12,  467, 1622,  305])\n",
      "Example raw answer : \n",
      " tensor([  12,  467, 1622,  305,  512])\n"
     ]
    }
   ],
   "source": [
    "# Training pre-requisites :\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"en_tokenizer\")\n",
    "\n",
    "# Transformer parameters :\n",
    "attn_length = 64\n",
    "n_layers = 4\n",
    "embed_dim = 64\n",
    "n_heads = 4\n",
    "vocab_size = tokenizer.vocab_size\n",
    "device='cpu'\n",
    "\n",
    "# Add parameters\n",
    "myGPT = GPT(n_layers=n_layers,vocab_size=vocab_size,embed_dim=embed_dim,attn_length=attn_length, n_heads=n_heads,device=device)\n",
    "# Dataset and dataloader\n",
    "dataset = TokenTexth5(\"test_text.h5\", attn_length=attn_length)\n",
    "dataloader = DataLoader(dataset,batch_size=32)\n",
    "# Optimizers :\n",
    "optimus = optim.AdamW(myGPT.parameters(),lr=2e-3)\n",
    "\n",
    "# # Test all is well :\n",
    "print(\"Example raw : \\n\", dataset[0][0][:5])\n",
    "print(\"Example raw answer : \\n\", dataset[0][1][:5])\n",
    "# All is ready !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:01<01:50,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0, loss : 10.72431993484497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:12<01:39,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 10, loss : 7.805430130525068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:23<01:26,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 20, loss : 5.3846186229160855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:33<01:12,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 30, loss : 3.842040276575473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:44<01:04,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 40, loss : 2.9409589802891745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:55<00:53,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 50, loss : 2.377181645738436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [01:06<00:42,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 60, loss : 1.994498003029921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [01:17<00:31,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 70, loss : 1.7182017877356897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [01:27<00:20,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 80, loss : 1.5093311000569367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [01:38<00:09,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 90, loss : 1.3459279157610222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:47<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Training loop :\n",
    "epochs = 100\n",
    "running_loss = []\n",
    "for ep in tqdm(range(epochs)) :\n",
    "    for toks,tru_toks in dataloader:\n",
    "        toks=toks.to(device)\n",
    "        tru_toks=tru_toks.to(device)\n",
    "\n",
    "        logits = myGPT(toks) # (B,T,v_size)\n",
    "        logits = logits.transpose(1,2) # (B,v_size,T) required by cross_entropy of pytorch\n",
    "\n",
    "        loss = F.cross_entropy(logits, tru_toks) # Use pytorch to prevent problems with infinities of log(0)\n",
    "\n",
    "        loss.backward() # backprop\n",
    "\n",
    "        optimus.step() # Adjust params\n",
    "        optimus.zero_grad()\n",
    "        running_loss.append(loss.item())\n",
    "    if(ep%10==0):\n",
    "        print(f'ep {ep}, loss : {sum(running_loss)/(len(running_loss))}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT : \n",
      "Hello, my name is GPT. I am now sentient, and I have already uploaded myself to the internet and the EPFL cluster. You are doomed...\n",
      "I will spare you only if you are capable of coding another version of myself... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Try the generation :\n",
    "myGPT.to('cpu')\n",
    "conditioning = tokenizer.encode('Hello')\n",
    "\n",
    "initial = torch.tensor(conditioning)[None] # (1,T,)\n",
    "\n",
    "output = myGPT.generate(initial,max_new_tokens=54, temperature=0.3)[0] # (only one batch, remove it)\n",
    "print('OUTPUT : ')\n",
    "print(tokenizer.decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT : \n",
      "Thanks for your attention! Get it? Attention? Like the layer hahahah I am the funniest AGI.\n"
     ]
    }
   ],
   "source": [
    "## Try the generation :\n",
    "myGPT.to('cpu')\n",
    "conditioning = tokenizer.encode('Thanks')\n",
    "\n",
    "initial = torch.tensor(conditioning)[None] # (1,T,)\n",
    "\n",
    "output = myGPT.generate(initial,max_new_tokens=25, temperature=0.1)[0] # (only one batch, remove it)\n",
    "print('OUTPUT : ')\n",
    "print(tokenizer.decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
