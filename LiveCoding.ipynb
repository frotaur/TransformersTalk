{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Transformers from scratch (almost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch,torch.nn as nn,torch.nn.functional as F, torch.optim as optim, math\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import TokenTexth5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, attn_length, n_heads, dropout=0.1,device='cpu'): # Define the sub-layers here\n",
    "        super().__init__()\n",
    "        self.attn_length = attn_length\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "\n",
    "    def forward(self, x): # Apply the different layers\n",
    "        \"\"\"\n",
    "            Args : \n",
    "            x : (B,T,D)\n",
    "\n",
    "            Return : Tensor (B,T,D)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_test =  MaskedSelfAttention(embed_dim=6,attn_length=10, n_heads = 2)\n",
    "\n",
    "in_tens = torch.randn((1,8,6))\n",
    "\n",
    "out_tens = attn_test(in_tens)# (B,T,D)\n",
    "\n",
    "assert out_tens.shape==in_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP layer, easy part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple feedforward with two layers. Blows up by a factor of 4 the embed_dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,embed_dim, mlp_ratio=4., device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Args :\n",
    "            x : (*,embed_dim)\n",
    "\n",
    "            Returns : (*,embed_dim)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmlp = MLP(embed_dim=5)\n",
    "\n",
    "test_input = torch.randn((2,10,5))\n",
    "\n",
    "print('input shape: ', test_input.shape)\n",
    "print('outshape : ', testmlp(test_input).shape)\n",
    "# print('output : ', testmlp(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Decoder' Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim,attn_length, n_heads, dropout=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args :\n",
    "        x : (B,T,D) tensor\n",
    "\n",
    "        Returns : (B,T,D) tensor\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_test =  TransformerBlock(embed_dim=6,attn_length=10, n_heads = 2)\n",
    "\n",
    "in_tens = torch.randn((2,8,6))\n",
    "\n",
    "out_tens = block_test(in_tens)# (B,T,D)\n",
    "\n",
    "assert out_tens.shape==in_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the text transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_layers, vocab_size,embed_dim,attn_length,n_heads, dropout=0.1,device='cpu'):\n",
    "        super().__init__()\n",
    "        self.attn_length = attn_length\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Args : \n",
    "            x : (B,T) of longs (tokens)\n",
    "\n",
    "            Returns : (B,T,vocab_size) of logits\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (B,T)) and complete\n",
    "            the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "            Use with model in inference mode (apply model.eval() first)\n",
    "\n",
    "            Args :\n",
    "            idx : (B,T) tensor of context tokens. Mostly, it will be B=1 but can do in parallel also\n",
    "            max_new_tokens : number of tokens to generate on top of the conditioning sequence\n",
    "            temperature : softmax temperature (lower -> more conservative sampling)\n",
    "            do_sample : if True, use multinomial sampling. Otherwise use greedy decoding\n",
    "            top_k : if set to int > 0, only sample from the top k most probable logits\n",
    "\n",
    "            Returns :\n",
    "            (B,T) LongTensor of generated token indices. Must still be decoded by tokenizer.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_next = self.generate_next_token(idx,temperature=temperature,do_sample=do_sample,top_k=top_k)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_next_token(self,idx,temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (B,T)) and return\n",
    "            the next predicted token.\n",
    "            Use with model in inference mode (apply model.eval() first)\n",
    "\n",
    "            Args :\n",
    "            idx : (B,T) tensor of context tokens. Mostly, it will be B=1 but can do in parallel also\n",
    "            temperature : softmax temperature (lower -> more conservative sampling)\n",
    "            do_sample : if True, use multinomial sampling. Otherwise use greedy decoding\n",
    "            top_k : if set to int > 0, only sample from the top k most probable logits\n",
    "\n",
    "            Returns :\n",
    "            next predicted token, Long\n",
    "        \"\"\"\n",
    "        idx_cond = idx if idx.shape[1] <= self.attn_length else idx[:, -self.attn_length:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits = self.forward(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            \n",
    "        # Return sampled index\n",
    "        return idx_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training pre-requisites :\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"en_tokenizer\")\n",
    "\n",
    "# Transformer parameters :\n",
    "attn_length = 64\n",
    "n_layers = 4\n",
    "embed_dim = 64\n",
    "n_heads = 4\n",
    "vocab_size = tokenizer.vocab_size\n",
    "device='cpu'\n",
    "\n",
    "# Add parameters\n",
    "myGPT = GPT(n_layers=n_layers,vocab_size=vocab_size,embed_dim=embed_dim,attn_length=attn_length, n_heads=n_heads,device=device)\n",
    "# Dataset and dataloader\n",
    "dataset = TokenTexth5(\"test_text.h5\", attn_length=attn_length)\n",
    "dataloader = DataLoader(dataset,batch_size=32)\n",
    "# Optimizers :\n",
    "optimus = optim.AdamW(myGPT.parameters(),lr=2e-3)\n",
    "\n",
    "# # Test all is well :\n",
    "print(\"Example raw : \\n\", dataset[0][0][:5])\n",
    "print(\"Example raw answer : \\n\", dataset[0][1][:5])\n",
    "# All is ready !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Training loop :\n",
    "epochs = 100\n",
    "running_loss = []\n",
    "for ep in tqdm(range(epochs)) :\n",
    "    for toks,tru_toks in dataloader:\n",
    "        toks=toks.to(device)\n",
    "        tru_toks=tru_toks.to(device)\n",
    "\n",
    "        logits = myGPT(toks) # (B,T,v_size)\n",
    "        logits = logits.transpose(1,2) # (B,v_size,T) required by cross_entropy of pytorch\n",
    "\n",
    "        loss = F.cross_entropy(logits, tru_toks) # Use pytorch to prevent problems with infinities of log(0)\n",
    "\n",
    "        loss.backward() # backprop\n",
    "\n",
    "        optimus.step() # Adjust params\n",
    "        optimus.zero_grad()\n",
    "        running_loss.append(loss.item())\n",
    "    if(ep%10==0):\n",
    "        print(f'ep {ep}, loss : {sum(running_loss)/(len(running_loss))}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try the generation :\n",
    "myGPT.to('cpu')\n",
    "conditioning = tokenizer.encode('Hello')\n",
    "\n",
    "initial = torch.tensor(conditioning)[None] # (1,T,)\n",
    "\n",
    "output = myGPT.generate(initial,max_new_tokens=54, temperature=0.3)[0] # (only one batch, remove it)\n",
    "print('OUTPUT : ')\n",
    "print(tokenizer.decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try the generation :\n",
    "myGPT.to('cpu')\n",
    "conditioning = tokenizer.encode('Thanks')\n",
    "\n",
    "initial = torch.tensor(conditioning)[None] # (1,T,)\n",
    "\n",
    "output = myGPT.generate(initial,max_new_tokens=25, temperature=0.1)[0] # (only one batch, remove it)\n",
    "print('OUTPUT : ')\n",
    "print(tokenizer.decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
