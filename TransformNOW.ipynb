{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Transformers from scratch (almost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os, torch,  torch.nn.functional as F, torch.optim as optim, math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import TokenTexth5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP layer, easy start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple feedforward with two layers. Blows up by a factor of 4 the embed_dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, mlp_ratio=4.,device='cpu'):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embed_dim, int(mlp_ratio*embed_dim), device=device)\n",
    "        self.non_lin = nn.GELU()\n",
    "        self.lin2 = nn.Linear(int(mlp_ratio*embed_dim),embed_dim,device=device)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Args :\n",
    "            x : (*,embed_dim)\n",
    "\n",
    "            Returns : (*,embed_dim)\n",
    "        \"\"\"\n",
    "        x= self.lin2(self.non_lin(self.lin1(x)))\n",
    "        \n",
    "        return x # (*,embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  tensor([[-1.1058, -0.7063, -1.3524, -3.1836,  0.2508],\n",
      "        [ 0.3831,  1.4820,  2.1111, -1.1022, -1.1278]])\n",
      "outshape :  torch.Size([2, 5])\n",
      "output :  tensor([[-0.5335, -0.2678, -0.4982, -0.1780, -0.1038],\n",
      "        [-0.0456,  0.2760, -0.3565, -0.0703, -0.0284]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testmlp = MLP(5)\n",
    "\n",
    "test_input = torch.randn((2,5))\n",
    "print('input : ', test_input)\n",
    "\n",
    "print('outshape : ', testmlp(test_input).shape)\n",
    "\n",
    "print('output : ', testmlp(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, attn_length, n_heads, dropout=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_length = attn_length\n",
    "        self.n_heads = n_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # QKV matrix makers\n",
    "        self.q_maker = nn.Linear(embed_dim,n_heads*(embed_dim),device=device)\n",
    "        self.k_maker = nn.Linear(embed_dim,n_heads*(embed_dim),device=device)\n",
    "        self.v_maker = nn.Linear(embed_dim,n_heads*(embed_dim), device=device)\n",
    "\n",
    "        self.project_out = nn.Linear(n_heads*embed_dim,embed_dim, device=device)\n",
    "        self.attn_dropout = nn.Dropout(0.1)\n",
    "        ## Define the mask\n",
    "        self.register_buffer(\"attn_mask\", torch.tril(torch.ones((attn_length,attn_length),device=device))==0) # Lower triangular of ones.\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Args : \n",
    "            x : (B,T,D)\n",
    "\n",
    "            Return : Tensor (B,T,D)\n",
    "        \"\"\"\n",
    "        B,T,D = x.shape\n",
    "\n",
    "        assert T<= self.attn_length, 'Input too long to fit in attention length'\n",
    "        assert D==self.embed_dim, 'Invalid embeddin dimensions !!'\n",
    "\n",
    "        ## Create the Q,K,V pairs, each of size (B,T,D)\n",
    "        K = self.k_maker(x).reshape(B,T,self.n_heads,D) # (B,T,n_heads,D_)\n",
    "        Q = self.q_maker(x).reshape(B,T,self.n_heads,D) # (B,T,n_heads,D_)\n",
    "        V = self.v_maker(x).reshape(B,T,self.n_heads,D) # (B,T,n_heads,D_)\n",
    "\n",
    "        Q = Q.permute(0,2,1,3) # (B,n_heads,T,D_)\n",
    "        K = K.permute(0,2,1,3) # (B,n_heads,T,D_)\n",
    "        V = V.permute(0,2,1,3) # (B,n_heads,T,D_)\n",
    "        \n",
    "        attention = (Q @ K.transpose(-1,-2))*(1./(math.sqrt(D))) # (B,n_heads,T,T)\n",
    "\n",
    "        # Apply the mask\n",
    "        attention = torch.masked_fill(attention, self.attn_mask[None,None,:T,:T],float('-inf'))\n",
    "\n",
    "        attention = F.softmax(attention,dim=-1)\n",
    "        attention = self.attn_dropout(attention)\n",
    "\n",
    "        attention = attention @ V # (B,n_heads,T,D_)\n",
    "        \n",
    "        attention = attention.permute(0,2,1,3) # (B,T,n_heads,D)\n",
    "        attention = attention.reshape(B,T,self.n_heads*D) # (B,T,n_heads*D)\n",
    "\n",
    "        return self.project_out(attention)\n",
    "    \n",
    "        # Attention[i,j] : q_i . k_j -> how much token i attends to token j\n",
    "        # Attention[i,j] = 0 if i<j. Tokens cannot attend future tokens.\n",
    "        ## Compute masked attention matrix\n",
    "\n",
    "        ## Project back and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_test =  MaskedSelfAttention(6,attn_length=10, n_heads = 2)\n",
    "\n",
    "in_tens = torch.randn((1,10,6))\n",
    "\n",
    "out_tens = attn_test(in_tens)# (B,T,D)\n",
    "\n",
    "assert out_tens.shape==in_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Decoder' Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim,attn_length, n_heads, dropout=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention_layer = MaskedSelfAttention(embed_dim=embed_dim,attn_length=attn_length,n_heads=n_heads, device=device)\n",
    "        self.feedforward = MLP(embed_dim=embed_dim,device=device)\n",
    "\n",
    "        self.attn_normalization = nn.LayerNorm(embed_dim,device=device)\n",
    "        self.mlp_normalization = nn.LayerNorm(embed_dim,device=device)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.mlp_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args :\n",
    "        x : (B,T,D) tensor\n",
    "\n",
    "        Returns : (B,T,D) tensor\n",
    "        \"\"\"\n",
    "        x = x+self.attn_dropout(self.attention_layer(self.attn_normalization(x)))\n",
    "        x = x+self.mlp_dropout(self.feedforward(self.mlp_normalization(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, the text transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_layers, vocab_size,embed_dim,attn_length,n_heads, dropout=0.1,device='cpu'):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size,embed_dim,device=device)\n",
    "        self.pos_embedder = nn.Embedding(attn_length,embed_dim,device=device)\n",
    "\n",
    "        self.embed_drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim,attn_length,n_heads,dropout,device) for _ in range(n_layers)])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(embed_dim,device=device)\n",
    "        self.project_out = nn.Linear(embed_dim, vocab_size, bias=False,device=device)\n",
    "\n",
    "        self.attn_length = attn_length\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Args : \n",
    "            x : (B,T) of longs (tokens)\n",
    "\n",
    "            Returns : (B,T,vocab_size) of logits\n",
    "        \"\"\"\n",
    "        ## First, do the positional and token embedding\n",
    "        B,T = x.shape\n",
    "\n",
    "        tokens = self.tok_embed(x) # (B,T,D)\n",
    "        positions = self.pos_embedder(torch.arange(0,T,1,dtype=torch.long,device=x.device))[None,...] # (1,T,D)\n",
    "\n",
    "        x = self.embed_drop(tokens+positions) # Sum the tokens and positions\n",
    "        # Ready to feed through the blocks !\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Last layernorm\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        x = self.project_out(x) #(B,T,v_size)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (B,T)) and complete\n",
    "            the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "            Use with model in inference mode (apply model.eval() first)\n",
    "\n",
    "            Args :\n",
    "            idx : (B,T) tensor of context tokens. Mostly, it will be B=1 but can do in parallel also\n",
    "            max_new_tokens : number of tokens to generate on top of the conditioning sequence\n",
    "            temperature : softmax temperature (lower -> more conservative sampling)\n",
    "            do_sample : if True, use multinomial sampling. Otherwise use greedy decoding\n",
    "            top_k : if set to int > 0, only sample from the top k most probable logits\n",
    "\n",
    "            Returns :\n",
    "            (B,T) LongTensor of generated token indices. Must still be decoded by tokenizer.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_next = self.generate_next_token(idx,temperature=temperature,do_sample=do_sample,top_k=top_k)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_next_token(self,idx,temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (B,T)) and return\n",
    "            the next predicted token.\n",
    "            Use with model in inference mode (apply model.eval() first)\n",
    "\n",
    "            Args :\n",
    "            idx : (B,T) tensor of context tokens. Mostly, it will be B=1 but can do in parallel also\n",
    "            temperature : softmax temperature (lower -> more conservative sampling)\n",
    "            do_sample : if True, use multinomial sampling. Otherwise use greedy decoding\n",
    "            top_k : if set to int > 0, only sample from the top k most probable logits\n",
    "\n",
    "            Returns :\n",
    "            next predicted token, Long\n",
    "        \"\"\"\n",
    "        idx_cond = idx if idx.shape[1] <= self.attn_length else idx[:, -self.attn_length:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits = self.forward(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            \n",
    "        # Return sampled index\n",
    "        return idx_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 0.00M tokens, resulting in 25 examples.\n",
      "Example raw : \n",
      " tensor([6279,   12,  467, 1622,  305])\n",
      "Example raw answer : \n",
      " tensor([  12,  467, 1622,  305,  512])\n"
     ]
    }
   ],
   "source": [
    "# Training pre-requisites :\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"en_tokenizer\")\n",
    "\n",
    "\n",
    "# Transformer parameters :\n",
    "attn_length = 128\n",
    "n_layers = 6\n",
    "embed_dim = 128\n",
    "n_heads = 4\n",
    "vocab_size = tokenizer.vocab_size\n",
    "device='cuda'\n",
    "\n",
    "myGPT = GPT(n_layers=n_layers,vocab_size=vocab_size,embed_dim=embed_dim,attn_length=attn_length, n_heads=n_heads,device=device)\n",
    "# Dataset and dataloader\n",
    "dataset = TokenTexth5(\"test_text.h5\", attn_length=attn_length)\n",
    "dataloader = DataLoader(dataset,batch_size=32)\n",
    "# Optimizers :\n",
    "optimus = optim.AdamW(myGPT.parameters(),lr=1e-3)\n",
    "\n",
    "# # Test all is well :\n",
    "# print(\"Example detokenized : \\n\", tokenizer.decode(dataset[0][0][:20]))\n",
    "# print(\"Example ground_truth : \\n\", tokenizer.decode(dataset[0][1][:20]))\n",
    "print(\"Example raw : \\n\", dataset[0][0][:5])\n",
    "print(\"Example raw answer : \\n\", dataset[0][1][:5])\n",
    "# All is ready !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:11,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0, loss : 3.9410483837127686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:03<00:08,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 30, loss : 1.434729645329137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:07<00:04,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 60, loss : 0.7900863991531192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:10<00:00,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 90, loss : 0.5437168239278125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Training loop :\n",
    "epochs = 100\n",
    "running_loss = []\n",
    "for ep in tqdm(range(epochs)) :\n",
    "    for toks,tru_toks in dataloader:\n",
    "        toks=toks.to(device)\n",
    "        tru_toks=tru_toks.to(device)\n",
    "\n",
    "        logits = myGPT(toks) # (B,T,v_size)\n",
    "        logits = logits.transpose(1,2) # (B,v_size,T) required by cross_entropy of pytorch\n",
    "\n",
    "        loss = F.cross_entropy(logits, tru_toks) # Use pytorch to prevent problems with infinities of log(0)\n",
    "\n",
    "        loss.backward() # backprop\n",
    "\n",
    "        optimus.step() # Adjust params\n",
    "        optimus.zero_grad()\n",
    "        running_loss.append(loss.item())\n",
    "    if(ep%30==0):\n",
    "        print(f'ep {ep}, loss : {sum(running_loss)/(len(running_loss))}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT : \n",
      "Hello, my name is GPT. I am now sentient, and I have already uploaded myself to the internet and the EPFL cluster. You are doomed...\n"
     ]
    }
   ],
   "source": [
    "## Try the generation :\n",
    "myGPT.to('cpu')\n",
    "initial = torch.tensor(tokenizer.encode('Hello'))[None] # (1,T,)\n",
    "\n",
    "output = myGPT.generate(initial,max_new_tokens=36)[0] # (only one batch, remove it)\n",
    "print('OUTPUT : ')\n",
    "print(tokenizer.decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
